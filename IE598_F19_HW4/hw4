import pandas as pd
import sys
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Lasso

# Read in file
df = pd.read_csv('housing2.csv')
df.head()
df.tail()

df=df.dropna()

X = df.iloc[:, :].values
y = df['MEDV'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)

slr = LinearRegression()
slr.fit(X_train, y_train)
y_train_pred = slr.predict(X_train)
y_test_pred = slr.predict(X_test)

plt.scatter(y_train_pred, y_train_pred - y_train, c = 'steelblue', 
            edgecolor = 'white', marker = 'o', s = 35, alpha = 0.9, label = 'Training data')
plt.scatter(y_test_pred, y_test_pred - y_test, c = 'limegreen', edgecolor = 'white', marker = 's', s = 35, alpha = 0.9, label = 'Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.legend(loc = 'upper left')
plt.hlines (y = 0, xmin = -10, xmax = 50, lw = 2, color = 'black')
plt.xlim([-10, 50])
plt.show()

# Create the regressor: reg_all
reg_all = LinearRegression()

# Fit the regressor to the training data
reg_all.fit(X_train,y_train)

# Predict on the test data: y_pred
y_pred = reg_all.predict(X_test)

print("Coefficients :{}".format(reg_all.coef_[0]))
print('Intercept: %.3f' % reg_all.intercept_)

# Compute and print R^2 and RMSE
print("R^2: {}".format(reg_all.score(X_test, y_test)))
rmse = np.sqrt(mean_squared_error(y_test,y_pred))
print("Root Mean Squared Error: {}".format(rmse))
